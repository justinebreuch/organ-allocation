{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, States to explore: 1\n",
      "Pruned to 315 states.\n",
      "Delta: 18.6\n",
      "Iteration 2, States to explore: 315\n",
      "Pruned to 5000 states.\n",
      "Delta: 18.6\n",
      "Iteration 3, States to explore: 5000\n",
      "Pruned to 5000 states.\n",
      "Delta: 18.6\n",
      "Iteration 4, States to explore: 5000\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class SequentialOrganAllocationMDP:\n",
    "    def __init__(self, initial_state, available_organs):\n",
    "        \"\"\"\n",
    "        Initialize the sequential MDP for organ allocation.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_state: List of dictionaries representing waitlist candidates.\n",
    "        - available_organs: Dictionary with available organs by blood type.\n",
    "        \"\"\"\n",
    "        self.initial_state = (\n",
    "            tuple((r['id'], r['age'], r['MELD'], r['blood_type'], r['allocated']) for r in initial_state),\n",
    "            tuple(available_organs.items())\n",
    "        )\n",
    "        self.value_table = {self.initial_state: 0}  # Value function initialized for the initial state\n",
    "        self.policy = {self.initial_state: None}  # Policy initialized for the initial state\n",
    "        self.deltas = []  # To store deltas over iterations\n",
    "\n",
    "    def calculate_reward(self, recipient):\n",
    "        \"\"\"\n",
    "        Calculate reward for successfully allocating an organ.\n",
    "\n",
    "        Parameters:\n",
    "        - recipient: Tuple representing the recipient receiving the organ.\n",
    "\n",
    "        Returns:\n",
    "        - Reward value based on MELD score and age.\n",
    "        \"\"\"\n",
    "        _, age, meld, _, _ = recipient\n",
    "        return 10 + (1 / (1 + max(meld,0))) * 10 - (age * 0.1) #this is giving inf rewards sometimes\n",
    "\n",
    "    def generate_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        Generate the next state based on the current state and action.\n",
    "\n",
    "        Parameters:\n",
    "        - state: Tuple (recipients, available_organs).\n",
    "        - action: Tuple (recipient_id, organ_type).\n",
    "\n",
    "        Returns:\n",
    "        - next_state: Updated state after taking the action.\n",
    "        - reward: Associated reward for the action.\n",
    "        \"\"\"\n",
    "        recipients, available_organs = state\n",
    "        recipients = list(recipients)\n",
    "        available_organs = dict(available_organs)\n",
    "\n",
    "        if action is None:\n",
    "            return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "        recipient_id, organ_type = action\n",
    "        recipient_idx = next((i for i, r in enumerate(recipients) if r[0] == recipient_id), None)\n",
    "\n",
    "        if recipient_idx is not None and available_organs[organ_type] > 0:\n",
    "            if random.random() < 0.8:  # 90% success\n",
    "                recipient = recipients[recipient_idx]\n",
    "                recipients[recipient_idx] = (recipient[0], recipient[1], recipient[2], recipient[3], 1)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = self.calculate_reward(recipient)\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "            else:  # 10% failure (recipient dies, organ is removed)\n",
    "                recipient = recipients.pop(recipient_idx)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = -100  # Large penalty for death\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "\n",
    "        return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "    '''\n",
    "    def value_iteration(self, gamma=0.9, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Perform value iteration for all reachable states starting from the initial state.\n",
    "\n",
    "        Parameters:\n",
    "        - gamma: Discount factor (default 0.9).\n",
    "        - epsilon: Convergence threshold (default 0.01).\n",
    "        \"\"\"\n",
    "        states_to_explore = deque([self.initial_state])\n",
    "        self.deltas = []  # Reset deltas at the start of value iteration\n",
    "        iter = 0\n",
    "        while True:  # Outer loop for global convergence\n",
    "            delta = 0  # Track the largest change in value across all states\n",
    "            iter += 1\n",
    "            print(f\"Iteration: {iter}\")\n",
    "            new_states_to_explore = deque()  # To track states added during this iteration\n",
    "\n",
    "            \n",
    "            while states_to_explore:  # Inner loop for processing current state\n",
    "                current_state = states_to_explore.popleft()\n",
    "                old_value = self.value_table[current_state]\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "                no_valid_actions = True\n",
    "\n",
    "                recipients, available_organs = current_state\n",
    "                available_organs = dict(available_organs)\n",
    "                for recipient in recipients:\n",
    "                    if recipient[4] == 0:  # Not yet allocated\n",
    "                        for organ_type in available_organs.keys():\n",
    "                            if available_organs[organ_type] > 0:\n",
    "                                action = (recipient[0], organ_type)\n",
    "                                next_state, reward = self.generate_next_state(current_state, action)\n",
    "                                value = reward + gamma * self.value_table.get(next_state, 0)\n",
    "                                no_valid_actions = False\n",
    "                                if np.abs(value) == float('inf') or np.abs(value) > 1e6:\n",
    "                                    print(f\"Large Reward: {reward}\")\n",
    "                                    print(f\"Curr state: {current_state}, Action: {action}, Next state: {next_state}\")\n",
    "\n",
    "\n",
    "\n",
    "                                if value > max_value:\n",
    "                                    max_value = value\n",
    "                                    best_action = action\n",
    "\n",
    "                                if next_state not in self.value_table:\n",
    "                                    self.value_table[next_state] = 0\n",
    "                                    self.policy[next_state] = None\n",
    "                                    new_states_to_explore.append(next_state)\n",
    "                if no_valid_actions:\n",
    "                    continue\n",
    "                self.value_table[current_state] = max_value\n",
    "                #print(max_value)\n",
    "                self.policy[current_state] = best_action\n",
    "                delta = max(delta, abs(old_value - max_value))\n",
    "                #delta = abs(old_value - max_value)\n",
    "            print(f\"Delta: delta\")\n",
    "            self.deltas.append(delta)  # Append delta for this iteration\n",
    "            states_to_explore = new_states_to_explore  # Add newly discovered states\n",
    "            #print(f\"New states to explore: {len(new_states_to_explore)}\")\n",
    "            print(f\"Total of states to explore: {len(states_to_explore)}\")\n",
    "\n",
    "            if delta < epsilon and not states_to_explore:\n",
    "                break  # Stop when values converge and no new states to explore\n",
    "    '''\n",
    "    def value_iteration(self, gamma=0.9, epsilon=0.01, max_states=5000):\n",
    "        \"\"\"\n",
    "        Perform value iteration with pruning.\n",
    "        \n",
    "        Parameters:\n",
    "        - gamma: Discount factor (default 0.9).\n",
    "        - epsilon: Convergence threshold (default 0.01).\n",
    "        - max_states: Maximum number of states to retain in the queue during pruning.\n",
    "        \"\"\"\n",
    "        states_to_explore = deque([self.initial_state])\n",
    "        visited_states = set()  # Track visited states\n",
    "        self.deltas = []  # Reset deltas at the start of value iteration\n",
    "        iteration_count = 0\n",
    "\n",
    "        while True:\n",
    "            iteration_count += 1\n",
    "            delta = 0\n",
    "            new_states_to_explore = []\n",
    "\n",
    "            print(f\"Iteration {iteration_count}, States to explore: {len(states_to_explore)}\")\n",
    "\n",
    "            while states_to_explore:\n",
    "                current_state = states_to_explore.popleft()\n",
    "\n",
    "                # Skip already visited states\n",
    "                if current_state in visited_states:\n",
    "                    continue\n",
    "\n",
    "                visited_states.add(current_state)\n",
    "                old_value = self.value_table[current_state]\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "                no_valid_actions = True\n",
    "\n",
    "                recipients, available_organs = current_state\n",
    "                available_organs = dict(available_organs)\n",
    "\n",
    "                for recipient in recipients:\n",
    "                    if recipient[4] == 0:  # Not yet allocated\n",
    "                        for organ_type in available_organs.keys():\n",
    "                            if available_organs[organ_type] > 0:\n",
    "                                no_valid_actions = False\n",
    "                                action = (recipient[0], organ_type)\n",
    "                                next_state, reward = self.generate_next_state(current_state, action)\n",
    "                                value = reward + gamma * self.value_table.get(next_state, 0)\n",
    "\n",
    "                                if value > max_value:\n",
    "                                    max_value = value\n",
    "                                    best_action = action\n",
    "\n",
    "                                if next_state not in self.value_table:\n",
    "                                    self.value_table[next_state] = 0\n",
    "                                    self.policy[next_state] = None\n",
    "                                    new_states_to_explore.append((next_state, abs(old_value - max_value)))\n",
    "                if no_valid_actions:\n",
    "                    continue\n",
    "                self.value_table[current_state] = max_value\n",
    "                self.policy[current_state] = best_action\n",
    "                delta = max(delta, abs(old_value - max_value))\n",
    "\n",
    "            # Prune new states\n",
    "            new_states_to_explore.sort(key=lambda x: x[1], reverse=True)  # Sort by delta (value change)\n",
    "            new_states_to_explore = [state for state, _ in new_states_to_explore[:max_states]]\n",
    "\n",
    "            print(f\"Pruned to {len(new_states_to_explore)} states.\")\n",
    "\n",
    "            states_to_explore = deque(new_states_to_explore)\n",
    "            self.deltas.append(delta)\n",
    "            print(f\"Delta: {delta}\")\n",
    "\n",
    "            if delta < epsilon and not states_to_explore:\n",
    "                break\n",
    "\n",
    "    def get_deltas(self):\n",
    "        \"\"\"\n",
    "        Retrieve the deltas recorded during value iteration.\n",
    "\n",
    "        Returns:\n",
    "        - List of delta values for each iteration.\n",
    "        \"\"\"\n",
    "        return self.deltas #To use for plotting later\n",
    "\n",
    "    def simulate_with_policy(self, steps=10):\n",
    "        \"\"\"\n",
    "        Simulate the allocation process using the computed policy.\n",
    "\n",
    "        Parameters:\n",
    "        - steps: Number of allocation steps to simulate (default 10).\n",
    "\n",
    "        Returns:\n",
    "        - Total reward, total deaths, total allocations.\n",
    "        \"\"\"\n",
    "        current_state = self.initial_state\n",
    "        total_reward = 0\n",
    "        total_deaths = 0\n",
    "        total_allocations = 0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            action = self.policy.get(current_state, None)\n",
    "            if action is None:\n",
    "                break\n",
    "\n",
    "            next_state, reward = self.generate_next_state(current_state, action)\n",
    "\n",
    "            if reward == -100:  # Death penalty\n",
    "                total_deaths += 1\n",
    "            elif reward > 0:  # Successful allocation\n",
    "                total_allocations += 1\n",
    "\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "\n",
    "        return total_reward, total_deaths, total_allocations\n",
    "\n",
    "\n",
    "# Reinitialize the example with a reduced state space for debugging\n",
    "df = pd.read_csv('waitlist_patients.csv')\n",
    "\n",
    "initial_state = df.apply(\n",
    "    lambda row: {\n",
    "        'id': row.name + 1,  # Generate an 'id' starting from 1\n",
    "        'age': row['RECIPIENT_AGE'],  # Replace with the actual age column if available\n",
    "        'MELD': row['INIT_MELD_PELD_LAB_SCORE'],\n",
    "        'blood_type': row['RECIPIENT_BLOOD_TYPE'],\n",
    "        'allocated': 0  # Default value\n",
    "    }, axis=1\n",
    ").tolist()\n",
    "available_organs = {'A': 1, 'A1': 6, 'A1B': 0, 'A2': 0, 'A2B': 2, 'AB': 0, 'B': 3, 'O': 4, 'AB': 0}\n",
    "'''\n",
    "initial_state = [\n",
    "    {'id': i, 'age': random.randint(20, 70), 'MELD': random.randint(10, 40), 'blood_type': random.choice(['A', 'B', 'O', 'AB']), 'allocated': 0}\n",
    "    for i in range(1, 6)\n",
    "]\n",
    "available_organs = {'A': 0, 'B': 1, 'O': 1, 'AB': 1}\n",
    "'''\n",
    "# Initialize and execute the MDP\n",
    "mdp_model = SequentialOrganAllocationMDP(initial_state, available_organs)\n",
    "mdp_model.value_iteration()\n",
    "deltas = mdp_model.get_deltas()\n",
    "total_reward, total_deaths, total_allocations = mdp_model.simulate_with_policy(steps=30)\n",
    "\n",
    "\n",
    "total_reward, total_deaths, total_allocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RECIPIENT_BLOOD_TYPE\n",
       "O     27\n",
       "A     20\n",
       "B     11\n",
       "AB     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RECIPIENT_BLOOD_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "18.6\n",
      "2\n",
      "18.6\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 194\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Initialize and execute the MDP\u001b[39;00m\n\u001b[0;32m    193\u001b[0m mdp_model \u001b[38;5;241m=\u001b[39m SequentialOrganAllocationMDP(initial_state, available_organs)\n\u001b[1;32m--> 194\u001b[0m mdp_model\u001b[38;5;241m.\u001b[39mvalue_iteration()\n\u001b[0;32m    195\u001b[0m deltas \u001b[38;5;241m=\u001b[39m mdp_model\u001b[38;5;241m.\u001b[39mget_deltas()\n\u001b[0;32m    196\u001b[0m total_reward, total_deaths, total_allocations \u001b[38;5;241m=\u001b[39m mdp_model\u001b[38;5;241m.\u001b[39msimulate_with_policy(steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[110], line 112\u001b[0m, in \u001b[0;36mSequentialOrganAllocationMDP.value_iteration\u001b[1;34m(self, gamma, epsilon)\u001b[0m\n\u001b[0;32m    109\u001b[0m     max_value \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    110\u001b[0m     best_action \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_table[next_state] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[next_state] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class SequentialOrganAllocationMDP:\n",
    "    def __init__(self, initial_state, available_organs):\n",
    "        \"\"\"\n",
    "        Initialize the sequential MDP for organ allocation.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_state: List of dictionaries representing waitlist candidates.\n",
    "        - available_organs: Dictionary with available organs by blood type.\n",
    "        \"\"\"\n",
    "        self.initial_state = (\n",
    "            tuple((r['id'], r['age'], r['MELD'], r['blood_type'], r['allocated']) for r in initial_state),\n",
    "            tuple(available_organs.items())\n",
    "        )\n",
    "        self.value_table = {self.initial_state: 0}  # Value function initialized for the initial state\n",
    "        self.policy = {self.initial_state: None}  # Policy initialized for the initial state\n",
    "        self.deltas = []  # To store deltas over iterations\n",
    "\n",
    "    def calculate_reward(self, recipient):\n",
    "        \"\"\"\n",
    "        Calculate reward for successfully allocating an organ.\n",
    "\n",
    "        Parameters:\n",
    "        - recipient: Tuple representing the recipient receiving the organ.\n",
    "\n",
    "        Returns:\n",
    "        - Reward value based on MELD score and age.\n",
    "        \"\"\"\n",
    "        _, age, meld, _, _ = recipient\n",
    "        return 10 + (1 / (1 + max(meld,0))) * 10 - (age * 0.1) #this is giving inf rewards sometimes\n",
    "\n",
    "    def generate_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        Generate the next state based on the current state and action.\n",
    "\n",
    "        Parameters:\n",
    "        - state: Tuple (recipients, available_organs).\n",
    "        - action: Tuple (recipient_id, organ_type).\n",
    "\n",
    "        Returns:\n",
    "        - next_state: Updated state after taking the action.\n",
    "        - reward: Associated reward for the action.\n",
    "        \"\"\"\n",
    "        recipients, available_organs = state\n",
    "        recipients = list(recipients)\n",
    "        available_organs = dict(available_organs)\n",
    "\n",
    "        if action is None:\n",
    "            return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "        recipient_id, organ_type = action\n",
    "        recipient_idx = next((i for i, r in enumerate(recipients) if r[0] == recipient_id), None)\n",
    "\n",
    "        if recipient_idx is not None and available_organs[organ_type] > 0:\n",
    "            if random.random() < 0.9:  # 90% success\n",
    "                recipient = recipients[recipient_idx]\n",
    "                recipients[recipient_idx] = (recipient[0], recipient[1], recipient[2], recipient[3], 1)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = self.calculate_reward(recipient)\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "            else:  # 10% failure (recipient dies, organ is removed)\n",
    "                recipient = recipients.pop(recipient_idx)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = -100  # Large penalty for death\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "\n",
    "        return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "    def value_iteration(self, gamma=0.9, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Perform value iteration for all reachable states starting from the initial state.\n",
    "\n",
    "        Parameters:\n",
    "        - gamma: Discount factor (default 0.9).\n",
    "        - epsilon: Convergence threshold (default 0.01).\n",
    "        \"\"\"\n",
    "        states_to_explore = deque([self.initial_state])\n",
    "        self.deltas = []  # Reset deltas at the start of value iteration\n",
    "        iter = 0\n",
    "        while True:  # Outer loop for global convergence\n",
    "            delta = 0  # Track the largest change in value across all states\n",
    "            iter += 1\n",
    "            print(iter)\n",
    "            new_states_to_explore = deque()  # To track states added during this iteration\n",
    "\n",
    "            while states_to_explore:  # Inner loop for processing current states\n",
    "                current_state = states_to_explore.popleft()\n",
    "                old_value = self.value_table[current_state]\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "                no_valid_actions = True\n",
    "\n",
    "                recipients, available_organs = current_state\n",
    "                available_organs = dict(available_organs)\n",
    "                for recipient in recipients:\n",
    "                    if recipient[4] == 0:  # Not yet allocated\n",
    "                        for organ_type in available_organs.keys():\n",
    "                            if available_organs[organ_type] > 0:\n",
    "                                action = (recipient[0], organ_type)\n",
    "                                next_state, reward = self.generate_next_state(current_state, action)\n",
    "                                value = reward + gamma * self.value_table.get(next_state, 0)\n",
    "                                no_valid_actions = False\n",
    "                                if value > max_value:\n",
    "                                    max_value = value\n",
    "                                    best_action = action\n",
    "\n",
    "                                if next_state not in self.value_table:\n",
    "                                    self.value_table[next_state] = 0\n",
    "                                    self.policy[next_state] = None\n",
    "                                    new_states_to_explore.append(next_state)\n",
    "                if no_valid_actions:\n",
    "                    #print(f\"No valid actions for state: {current_state}\")\n",
    "                    continue\n",
    "\n",
    "                self.value_table[current_state] = max_value\n",
    "                #print(max_value)\n",
    "                self.policy[current_state] = best_action\n",
    "                delta = max(delta, abs(old_value - max_value))\n",
    "                #delta = abs(old_value - max_value)\n",
    "\n",
    "                \n",
    "            print(delta)\n",
    "            self.deltas.append(delta)  # Append delta for this iteration\n",
    "            states_to_explore = new_states_to_explore  # Add newly discovered states\n",
    "\n",
    "            if delta < epsilon and not states_to_explore:\n",
    "                print(states_to_explore)\n",
    "                break  # Stop when values converge and no new states to explore\n",
    "\n",
    "    def get_deltas(self):\n",
    "        \"\"\"\n",
    "        Retrieve the deltas recorded during value iteration.\n",
    "\n",
    "        Returns:\n",
    "        - List of delta values for each iteration.\n",
    "        \"\"\"\n",
    "        return self.deltas #To use for plotting later\n",
    "\n",
    "    def simulate_with_policy(self, steps=10):\n",
    "        \"\"\"\n",
    "        Simulate the allocation process using the computed policy.\n",
    "\n",
    "        Parameters:\n",
    "        - steps: Number of allocation steps to simulate (default 10).\n",
    "\n",
    "        Returns:\n",
    "        - Total reward, total deaths, total allocations.\n",
    "        \"\"\"\n",
    "        current_state = self.initial_state\n",
    "        total_reward = 0\n",
    "        total_deaths = 0\n",
    "        total_allocations = 0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            action = self.policy.get(current_state, None)\n",
    "            if action is None:\n",
    "                break\n",
    "\n",
    "            next_state, reward = self.generate_next_state(current_state, action)\n",
    "\n",
    "            if reward == -100:  # Death penalty\n",
    "                total_deaths += 1\n",
    "            elif reward > 0:  # Successful allocation\n",
    "                total_allocations += 1\n",
    "\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "\n",
    "        return total_reward, total_deaths, total_allocations\n",
    "\n",
    "\n",
    "# Reinitialize the example with a reduced state space for debugging\n",
    "np.random.seed(123)\n",
    "df = pd.read_csv('waitlist_patients.csv')\n",
    "\n",
    "initial_state = df.apply(\n",
    "    lambda row: {\n",
    "        'id': row.name + 1,  # Generate an 'id' starting from 1\n",
    "        'age': row['RECIPIENT_AGE'],  # Replace with the actual age column if available\n",
    "        'MELD': row['INIT_MELD_PELD_LAB_SCORE'],\n",
    "        'blood_type': row['RECIPIENT_BLOOD_TYPE'],\n",
    "        'allocated': 0  # Default value\n",
    "    }, axis=1\n",
    ").tolist()\n",
    "available_organs = {'A': 1, 'A1': 6, 'A1B': 0, 'A2': 0, 'A2B': 2, 'AB': 0, 'B': 3, 'O': 4, 'AB': 0}\n",
    "\n",
    "# Initialize and execute the MDP\n",
    "mdp_model = SequentialOrganAllocationMDP(initial_state, available_organs)\n",
    "mdp_model.value_iteration()\n",
    "deltas = mdp_model.get_deltas()\n",
    "total_reward, total_deaths, total_allocations = mdp_model.simulate_with_policy(steps=10)\n",
    "\n",
    "# Output the total reward, total deaths, and total allocations\n",
    "print(total_reward, total_deaths, total_allocations, deltas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class SequentialOrganAllocationMDP:\n",
    "    def __init__(self, initial_state, available_organs):\n",
    "        \"\"\"\n",
    "        Initialize the sequential MDP for organ allocation.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_state: List of dictionaries representing waitlist candidates.\n",
    "        - available_organs: Dictionary with available organs by blood type.\n",
    "        \"\"\"\n",
    "        self.initial_state = (\n",
    "            tuple((r['id'], r['age'], r['MELD'], r['blood_type'], r['allocated']) for r in initial_state),\n",
    "            tuple(available_organs.items())\n",
    "        )\n",
    "        self.value_table = {self.initial_state: 0}  # Value function initialized for the initial state\n",
    "        self.policy = {self.initial_state: None}  # Policy initialized for the initial state\n",
    "        self.deltas = []  # To store deltas over iterations\n",
    "\n",
    "    def calculate_reward(self, recipient):\n",
    "        _, age, meld, _, _ = recipient\n",
    "        if meld < 0 or age < 0:\n",
    "            print(f\"Warning: Invalid MELD or age value. MELD: {meld}, Age: {age}\")\n",
    "        reward = 10 + (1 / (1 + max(meld, 0))) * 10 - (age * 0.1)\n",
    "        if reward > 100 or reward < -100:  # Arbitrary reward bounds\n",
    "            print(f\"Warning: Extreme reward value: {reward} for recipient {recipient}\")\n",
    "        return reward\n",
    "\n",
    "    def generate_next_state(self, state, action):\n",
    "        recipients, available_organs = state\n",
    "        recipients = list(recipients)\n",
    "        available_organs = dict(available_organs)\n",
    "\n",
    "        if action is None:\n",
    "            return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "        recipient_id, organ_type = action\n",
    "        recipient_idx = next((i for i, r in enumerate(recipients) if r[0] == recipient_id), None)\n",
    "\n",
    "        if recipient_idx is not None and available_organs[organ_type] > 0:\n",
    "            if random.random() < 0.9:  # 70% success\n",
    "                recipient = recipients[recipient_idx]\n",
    "                recipients[recipient_idx] = (recipient[0], recipient[1], recipient[2], recipient[3], 1)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = self.calculate_reward(recipient)\n",
    "                print(f\"Action success: Recipient {recipient_id} allocated organ {organ_type}. Reward: {reward}\")\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "            else:  # 30% failure (recipient dies, organ is removed)\n",
    "                recipient = recipients.pop(recipient_idx)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = -100  # Large penalty for death\n",
    "                print(f\"Action failure: Recipient {recipient_id} died. Organ {organ_type} removed. Reward: {reward}\")\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "\n",
    "        print(f\"Invalid action: No organ {organ_type} available or recipient {recipient_id} not found.\")\n",
    "        return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "\n",
    "    def value_iteration(self, gamma=0.9, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Perform value iteration for all reachable states starting from the initial state.\n",
    "        \"\"\"\n",
    "        states_to_explore = deque([self.initial_state])\n",
    "        self.deltas = []  # Reset deltas at the start of value iteration\n",
    "        iteration_count = 0\n",
    "\n",
    "        while True:  # Outer loop for global convergence\n",
    "            iteration_count += 1\n",
    "            delta = 0  # Track the largest change in value across all states\n",
    "            new_states_to_explore = deque()  # To track states added during this iteration\n",
    "\n",
    "            while states_to_explore:  # Inner loop for processing current states\n",
    "                current_state = states_to_explore.popleft()\n",
    "                old_value = self.value_table[current_state]\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "\n",
    "                recipients, available_organs = current_state\n",
    "                available_organs = dict(available_organs)\n",
    "                no_valid_actions = True\n",
    "                for recipient in recipients:\n",
    "                    if recipient[4] == 0:  # Not yet allocated\n",
    "                        for organ_type in available_organs.keys():\n",
    "                            if available_organs[organ_type] > 0:\n",
    "                                action = (recipient[0], organ_type)\n",
    "                                next_state, reward = self.generate_next_state(current_state, action)\n",
    "                                no_valid_actions = False\n",
    "                                # Log state details during the last iterations\n",
    "                                if iteration_count > 5:  # Customize as needed\n",
    "                                    print(f\"Iteration {iteration_count}, State: {current_state}\")\n",
    "                                    print(f\"Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
    "                                \n",
    "                                value = reward + gamma * self.value_table.get(next_state, 0)\n",
    "                                if value > max_value:\n",
    "                                    max_value = value\n",
    "                                    best_action = action\n",
    "\n",
    "                                if next_state not in self.value_table:\n",
    "                                    self.value_table[next_state] = 0\n",
    "                                    self.policy[next_state] = None\n",
    "                                    new_states_to_explore.append(next_state)\n",
    "                if no_valid_actions:\n",
    "                    #print(f\"No valid actions for state: {current_state}\")\n",
    "                    continue\n",
    "\n",
    "                self.value_table[current_state] = max_value\n",
    "                self.policy[current_state] = best_action\n",
    "                delta = max(delta, abs(old_value - max_value))\n",
    "            print(delta)\n",
    "            self.deltas.append(delta)  # Append delta for this iteration\n",
    "            states_to_explore = new_states_to_explore  # Add newly discovered states\n",
    "\n",
    "            # Log delta during last few iterations\n",
    "            if iteration_count > 5:  # Adjust the threshold for logging\n",
    "                print(f\"Iteration {iteration_count}, Delta: {delta}\")\n",
    "\n",
    "            if delta < epsilon and not states_to_explore:\n",
    "                break  # Stop when values converge and no new states to explore\n",
    "\n",
    "\n",
    "    def get_deltas(self):\n",
    "        \"\"\"\n",
    "        Retrieve the deltas recorded during value iteration.\n",
    "\n",
    "        Returns:\n",
    "        - List of delta values for each iteration.\n",
    "        \"\"\"\n",
    "        return self.deltas #To use for plotting later\n",
    "\n",
    "    def simulate_with_policy(self, steps=10):\n",
    "        \"\"\"\n",
    "        Simulate the allocation process using the computed policy.\n",
    "\n",
    "        Parameters:\n",
    "        - steps: Number of allocation steps to simulate (default 10).\n",
    "\n",
    "        Returns:\n",
    "        - Total reward, total deaths, total allocations.\n",
    "        \"\"\"\n",
    "        current_state = self.initial_state\n",
    "        total_reward = 0\n",
    "        total_deaths = 0\n",
    "        total_allocations = 0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            action = self.policy.get(current_state, None)\n",
    "            if action is None:\n",
    "                break\n",
    "\n",
    "            next_state, reward = self.generate_next_state(current_state, action)\n",
    "\n",
    "            if reward == -100:  # Death penalty\n",
    "                total_deaths += 1\n",
    "            elif reward > 0:  # Successful allocation\n",
    "                total_allocations += 1\n",
    "\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "\n",
    "        return total_reward, total_deaths, total_allocations\n",
    "\n",
    "\n",
    "# Reinitialize the example with a reduced state space for debugging\n",
    "np.random.seed(123)\n",
    "initial_state = [\n",
    "    {'id': i, 'age': random.randint(20, 70), 'MELD': random.randint(10, 40), 'blood_type': random.choice(['A', 'A1', 'A1B' 'A2', 'A2B', 'AB', 'B', 'O', 'AB']), 'allocated': 0}\n",
    "    for i in range(1, 6)\n",
    "]\n",
    "df = pd.read_csv('waitlist_patients.csv')\n",
    "\n",
    "initial_state = df.apply(\n",
    "    lambda row: {\n",
    "        'id': row.name + 1,  # Generate an 'id' starting from 1\n",
    "        'age': row['RECIPIENT_AGE'],  # Replace with the actual age column if available\n",
    "        'MELD': row['INIT_MELD_PELD_LAB_SCORE'],\n",
    "        'blood_type': row['RECIPIENT_BLOOD_TYPE'],\n",
    "        'allocated': 0  # Default value\n",
    "    }, axis=1\n",
    ").tolist()\n",
    "available_organs = {'A': 1, 'A1': 6, 'A1B': 0, 'A2': 0, 'A2B': 2, 'AB': 0, 'B': 3, 'O': 4, 'AB': 0}\n",
    "available_organs = {'A': 1, 'A1': 6, 'A1B': 0, 'A2': 0, 'A2B': 2, 'AB': 0, 'B': 3, 'O': 4, 'AB': 0}\n",
    "#available_organs = {'A': 1, 'A1': 1, 'A1B': 2, 'A2': 0, 'A2B': 0, 'AB': 0, 'B': 0, 'O': 0, 'AB': 0}\n",
    "# Initialize and execute the MDP\n",
    "mdp_model = SequentialOrganAllocationMDP(initial_state, available_organs)\n",
    "mdp_model.value_iteration()\n",
    "deltas = mdp_model.get_deltas()\n",
    "total_reward, total_deaths, total_allocations = mdp_model.simulate_with_policy(steps=10)\n",
    "\n",
    "# Output the total reward, total deaths, and total allocations\n",
    "print(total_reward, total_deaths, total_allocations)\n",
    "\n",
    "total_reward, total_deaths, total_allocations = mdp_model.simulate_with_policy(steps=100)\n",
    "print(total_reward, total_deaths, total_allocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.555555555555555, 7.555555555555555, 7.555555555555555, 50.0, 0]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1380952380952367"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(self, gamma=0.9, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Perform value iteration for all reachable states starting from the initial state.\n",
    "    \"\"\"\n",
    "    states_to_explore = deque([self.initial_state])\n",
    "    self.deltas = []  # Reset deltas at the start of value iteration\n",
    "    iteration_count = 0\n",
    "\n",
    "    while True:  # Outer loop for global convergence\n",
    "        iteration_count += 1\n",
    "        delta = 0  # Track the largest change in value across all states\n",
    "        new_states_to_explore = deque()  # To track states added during this iteration\n",
    "\n",
    "        while states_to_explore:  # Inner loop for processing current states\n",
    "            current_state = states_to_explore.popleft()\n",
    "            old_value = self.value_table[current_state]\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            recipients, available_organs = current_state\n",
    "            available_organs = dict(available_organs)\n",
    "            for recipient in recipients:\n",
    "                if recipient[4] == 0:  # Not yet allocated\n",
    "                    for organ_type in available_organs.keys():\n",
    "                        if available_organs[organ_type] > 0:\n",
    "                            action = (recipient[0], organ_type)\n",
    "                            next_state, reward = self.generate_next_state(current_state, action)\n",
    "                            \n",
    "                            # Log state details during the last iterations\n",
    "                            if iteration_count > 5:  # Customize as needed\n",
    "                                print(f\"Iteration {iteration_count}, State: {current_state}\")\n",
    "                                print(f\"Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
    "                            \n",
    "                            value = reward + gamma * self.value_table.get(next_state, 0)\n",
    "                            if value > max_value:\n",
    "                                max_value = value\n",
    "                                best_action = action\n",
    "\n",
    "                            if next_state not in self.value_table:\n",
    "                                self.value_table[next_state] = 0\n",
    "                                self.policy[next_state] = None\n",
    "                                new_states_to_explore.append(next_state)\n",
    "\n",
    "            self.value_table[current_state] = max_value\n",
    "            self.policy[current_state] = best_action\n",
    "            delta = max(delta, abs(old_value - max_value))\n",
    "\n",
    "        self.deltas.append(delta)  # Append delta for this iteration\n",
    "        states_to_explore = new_states_to_explore  # Add newly discovered states\n",
    "\n",
    "        # Log delta during last few iterations\n",
    "        if iteration_count > 5:  # Adjust the threshold for logging\n",
    "            print(f\"Iteration {iteration_count}, Delta: {delta}\")\n",
    "\n",
    "        if delta < epsilon and not states_to_explore:\n",
    "            break  # Stop when values converge and no new states to explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_state(self, state, action):\n",
    "    recipients, available_organs = state\n",
    "    recipients = list(recipients)\n",
    "    available_organs = dict(available_organs)\n",
    "\n",
    "    if action is None:\n",
    "        return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "    recipient_id, organ_type = action\n",
    "    recipient_idx = next((i for i, r in enumerate(recipients) if r[0] == recipient_id), None)\n",
    "\n",
    "    if recipient_idx is not None and available_organs[organ_type] > 0:\n",
    "        if random.random() < 0.7:  # 70% success\n",
    "            recipient = recipients[recipient_idx]\n",
    "            recipients[recipient_idx] = (recipient[0], recipient[1], recipient[2], recipient[3], 1)\n",
    "            available_organs[organ_type] -= 1\n",
    "            reward = self.calculate_reward(recipient)\n",
    "            print(f\"Action success: Recipient {recipient_id} allocated organ {organ_type}. Reward: {reward}\")\n",
    "            return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "        else:  # 30% failure (recipient dies, organ is removed)\n",
    "            recipient = recipients.pop(recipient_idx)\n",
    "            available_organs[organ_type] -= 1\n",
    "            reward = -100  # Large penalty for death\n",
    "            print(f\"Action failure: Recipient {recipient_id} died. Organ {organ_type} removed. Reward: {reward}\")\n",
    "            return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "\n",
    "    print(f\"Invalid action: No organ {organ_type} available or recipient {recipient_id} not found.\")\n",
    "    return (tuple(recipients), tuple(available_organs.items())), 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('waitlist_patients.csv')\n",
    "\n",
    "initial_state = df.apply(\n",
    "    lambda row: {\n",
    "        'id': row.name + 1,  # Generate an 'id' starting from 1\n",
    "        'age': row['RECIPIENT_AGE'],  # Replace with the actual age column if available\n",
    "        'MELD': row['INIT_MELD_PELD_LAB_SCORE'],\n",
    "        'blood_type': row['RECIPIENT_BLOOD_TYPE'],\n",
    "        'allocated': 0  # Default value\n",
    "    }, axis=1\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'age': 62, 'MELD': 36, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 2, 'age': 61, 'MELD': 16, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 3, 'age': 46, 'MELD': 16, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 4, 'age': 57, 'MELD': 18, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 5, 'age': 46, 'MELD': 21, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 6, 'age': 59, 'MELD': 14, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 7, 'age': 67, 'MELD': 47, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 8, 'age': 68, 'MELD': 19, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 9, 'age': 52, 'MELD': 28, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 10, 'age': 57, 'MELD': 7, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 11, 'age': 53, 'MELD': 41, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 12, 'age': 67, 'MELD': 13, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 13, 'age': 14, 'MELD': -8, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 14, 'age': 14, 'MELD': 6, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 15, 'age': 52, 'MELD': 23, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 16, 'age': 65, 'MELD': 17, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 17, 'age': 55, 'MELD': 24, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 18, 'age': 51, 'MELD': 17, 'blood_type': 'AB', 'allocated': 0},\n",
       " {'id': 19, 'age': 73, 'MELD': 6, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 20, 'age': 58, 'MELD': 13, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 21, 'age': 35, 'MELD': 30, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 22, 'age': 65, 'MELD': 8, 'blood_type': 'AB', 'allocated': 0},\n",
       " {'id': 23, 'age': 56, 'MELD': 37, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 24, 'age': 67, 'MELD': 24, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 25, 'age': 45, 'MELD': 32, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 26, 'age': 45, 'MELD': 29, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 27, 'age': 57, 'MELD': 33, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 28, 'age': 62, 'MELD': 17, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 29, 'age': 51, 'MELD': 44, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 30, 'age': 47, 'MELD': 34, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 31, 'age': 45, 'MELD': 35, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 32, 'age': 30, 'MELD': 29, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 33, 'age': 49, 'MELD': 33, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 34, 'age': 51, 'MELD': 20, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 35, 'age': 54, 'MELD': 29, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 36, 'age': 61, 'MELD': 25, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 37, 'age': 54, 'MELD': 40, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 38, 'age': 47, 'MELD': 25, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 39, 'age': 25, 'MELD': 45, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 40, 'age': 49, 'MELD': 33, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 41, 'age': 46, 'MELD': 22, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 42, 'age': 62, 'MELD': 34, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 43, 'age': 52, 'MELD': 38, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 44, 'age': 36, 'MELD': 38, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 45, 'age': 48, 'MELD': 27, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 46, 'age': 62, 'MELD': 32, 'blood_type': 'AB', 'allocated': 0},\n",
       " {'id': 47, 'age': 43, 'MELD': 36, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 48, 'age': 44, 'MELD': 38, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 49, 'age': 40, 'MELD': 38, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 50, 'age': 71, 'MELD': 41, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 51, 'age': 63, 'MELD': 19, 'blood_type': 'AB', 'allocated': 0},\n",
       " {'id': 52, 'age': 54, 'MELD': 26, 'blood_type': 'AB', 'allocated': 0},\n",
       " {'id': 53, 'age': 53, 'MELD': 41, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 54, 'age': 65, 'MELD': 22, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 55, 'age': 71, 'MELD': 26, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 56, 'age': 54, 'MELD': 16, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 57, 'age': 45, 'MELD': 18, 'blood_type': 'B', 'allocated': 0},\n",
       " {'id': 58, 'age': 66, 'MELD': 40, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 59, 'age': 35, 'MELD': 24, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 60, 'age': 3, 'MELD': 58, 'blood_type': 'O', 'allocated': 0},\n",
       " {'id': 61, 'age': 61, 'MELD': 36, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 62, 'age': 68, 'MELD': 15, 'blood_type': 'A', 'allocated': 0},\n",
       " {'id': 63, 'age': 67, 'MELD': 18, 'blood_type': 'A', 'allocated': 0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert DataFrame to the desired format\n",
    "initial_state = df.apply(\n",
    "    lambda row: {\n",
    "        'id': row.name + 1,  # Generate an 'id' starting from 1\n",
    "        'age': row['RECIPIENT_AGE'],  # Replace with the actual age column if available\n",
    "        'MELD': row['INIT_MELD_PELD_LAB_SCORE'],\n",
    "        'blood_type': row['RECIPIENT_BLOOD_TYPE'],\n",
    "        'allocated': 0  # Default value\n",
    "    }, axis=1\n",
    ").tolist()\n",
    "\n",
    "result_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
