{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.51204481792717, 0, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class SequentialOrganAllocationMDP:\n",
    "    def __init__(self, initial_state, available_organs):\n",
    "        \"\"\"\n",
    "        Initialize the sequential MDP for organ allocation.\n",
    "\n",
    "        Parameters:\n",
    "        - initial_state: List of dictionaries representing waitlist candidates.\n",
    "        - available_organs: Dictionary with available organs by blood type.\n",
    "        \"\"\"\n",
    "        self.initial_state = (\n",
    "            tuple((r['id'], r['age'], r['MELD'], r['blood_type'], r['allocated']) for r in initial_state),\n",
    "            tuple(available_organs.items())\n",
    "        )\n",
    "        self.value_table = {self.initial_state: 0}  # Value function initialized for the initial state\n",
    "        self.policy = {self.initial_state: None}  # Policy initialized for the initial state\n",
    "\n",
    "    def calculate_reward(self, recipient):\n",
    "        \"\"\"\n",
    "        Calculate reward for successfully allocating an organ.\n",
    "\n",
    "        Parameters:\n",
    "        - recipient: Tuple representing the recipient receiving the organ.\n",
    "\n",
    "        Returns:\n",
    "        - Reward value based on MELD score and age.\n",
    "        \"\"\"\n",
    "        _, age, meld, _, _ = recipient\n",
    "        return 10 + (1 / (1 + meld)) * 10 - (age * 0.1)\n",
    "\n",
    "    def generate_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        Generate the next state based on the current state and action.\n",
    "\n",
    "        Parameters:\n",
    "        - state: Tuple (recipients, available_organs).\n",
    "        - action: Tuple (recipient_id, organ_type).\n",
    "\n",
    "        Returns:\n",
    "        - next_state: Updated state after taking the action.\n",
    "        - reward: Associated reward for the action.\n",
    "        \"\"\"\n",
    "        recipients, available_organs = state\n",
    "        recipients = list(recipients)\n",
    "        available_organs = dict(available_organs)\n",
    "\n",
    "        if action is None:\n",
    "            return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "        recipient_id, organ_type = action\n",
    "        recipient_idx = next((i for i, r in enumerate(recipients) if r[0] == recipient_id), None)\n",
    "\n",
    "        if recipient_idx is not None and available_organs[organ_type] > 0:\n",
    "            if random.random() < 0.7:  # 70% success\n",
    "                recipient = recipients[recipient_idx]\n",
    "                recipients[recipient_idx] = (recipient[0], recipient[1], recipient[2], recipient[3], 1)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = self.calculate_reward(recipient)\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "            else:  # 30% failure (recipient dies, organ is removed)\n",
    "                recipient = recipients.pop(recipient_idx)\n",
    "                available_organs[organ_type] -= 1\n",
    "                reward = -100  # Large penalty for death\n",
    "                return (tuple(recipients), tuple(available_organs.items())), reward\n",
    "\n",
    "        return (tuple(recipients), tuple(available_organs.items())), 0\n",
    "\n",
    "    def value_iteration(self, gamma=0.9, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Perform value iteration for all reachable states starting from the initial state.\n",
    "\n",
    "        Parameters:\n",
    "        - gamma: Discount factor (default 0.9).\n",
    "        - epsilon: Convergence threshold (default 0.01).\n",
    "        \"\"\"\n",
    "        states_to_explore = deque([self.initial_state])\n",
    "\n",
    "        while True:  # Outer loop for global convergence\n",
    "            delta = 0  # Track the largest change in value across all states\n",
    "            new_states_to_explore = deque()  # To track states added during this iteration\n",
    "\n",
    "            while states_to_explore:  # Inner loop for processing current states\n",
    "                current_state = states_to_explore.popleft()\n",
    "                old_value = self.value_table[current_state]\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "\n",
    "                recipients, available_organs = current_state\n",
    "                available_organs = dict(available_organs)\n",
    "                for recipient in recipients:\n",
    "                    if recipient[4] == 0:  # Not yet allocated\n",
    "                        for organ_type in available_organs.keys():\n",
    "                            if available_organs[organ_type] > 0:\n",
    "                                action = (recipient[0], organ_type)\n",
    "                                next_state, reward = self.generate_next_state(current_state, action)\n",
    "                                value = reward + gamma * self.value_table.get(next_state, 0)\n",
    "\n",
    "                                if value > max_value:\n",
    "                                    max_value = value\n",
    "                                    best_action = action\n",
    "\n",
    "                                if next_state not in self.value_table:\n",
    "                                    self.value_table[next_state] = 0\n",
    "                                    self.policy[next_state] = None\n",
    "                                    new_states_to_explore.append(next_state)\n",
    "\n",
    "                self.value_table[current_state] = max_value\n",
    "                self.policy[current_state] = best_action\n",
    "                delta = max(delta, abs(old_value - max_value))\n",
    "\n",
    "            states_to_explore = new_states_to_explore  # Add newly discovered states\n",
    "\n",
    "            if delta < epsilon and not states_to_explore:\n",
    "                break  # Stop when values converge and no new states to explore\n",
    "\n",
    "    def simulate_with_policy(self, steps=10):\n",
    "        \"\"\"\n",
    "        Simulate the allocation process using the computed policy.\n",
    "\n",
    "        Parameters:\n",
    "        - steps: Number of allocation steps to simulate (default 10).\n",
    "\n",
    "        Returns:\n",
    "        - Total reward, total deaths, total allocations.\n",
    "        \"\"\"\n",
    "        current_state = self.initial_state\n",
    "        total_reward = 0\n",
    "        total_deaths = 0\n",
    "        total_allocations = 0\n",
    "\n",
    "        for _ in range(steps):\n",
    "            action = self.policy.get(current_state, None)\n",
    "            if action is None:\n",
    "                break\n",
    "\n",
    "            next_state, reward = self.generate_next_state(current_state, action)\n",
    "\n",
    "            if reward == -100:  # Death penalty\n",
    "                total_deaths += 1\n",
    "            elif reward > 0:  # Successful allocation\n",
    "                total_allocations += 1\n",
    "\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "\n",
    "        return total_reward, total_deaths, total_allocations\n",
    "\n",
    "\n",
    "# Reinitialize the example with a reduced state space for debugging\n",
    "initial_state = [\n",
    "    {'id': i, 'age': random.randint(20, 70), 'MELD': random.randint(10, 40), 'blood_type': random.choice(['A', 'B', 'O', 'AB']), 'allocated': 0}\n",
    "    for i in range(1, 6)\n",
    "]\n",
    "available_organs = {'A': 0, 'B': 1, 'O': 1, 'AB': 1}\n",
    "\n",
    "# Initialize and execute the MDP\n",
    "mdp_model = SequentialOrganAllocationMDP(initial_state, available_organs)\n",
    "mdp_model.value_iteration()\n",
    "total_reward, total_deaths, total_allocations = mdp_model.simulate_with_policy(steps=10)\n",
    "\n",
    "# Output the total reward, total deaths, and total allocations\n",
    "total_reward, total_deaths, total_allocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
